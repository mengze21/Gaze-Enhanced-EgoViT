\chapter{Summary}
\label{chap:summary}

\section{Conclusion}
\label{sec:conclusion}
In this thesis, a Gaze-Enhanced \gls{dctg} module for the EgoViT model was proposed to improve action recognition performance in egocentric videos. The Gaze-Enhanced \gls{dctg} is designed to focus on additional regions of interest in the video frames, identified by gaze tracking points and detected hands and objects. Compared to the original EgoViT, the proposed Gaze-Enhanced EgoViT incorporates gaze tracking points as an additional input to the model. The Gaze-Enhanced \gls{dctg} module extracts gaze features from the gaze tracking points and input frames, merging them with hand-object features from a modified \gls{hod} module to generate dynamic class tokens. These dynamic class tokens guide the model to focus on regions enriched with gaze and hand-object information.

The proposed Gaze-Enhanced EgoViT model retains a similar architecture to the original EgoViT model, consisting of $G$ groups in the short-term stage, a class token Merging module, and a long-term stage. This architecture effectively handles temporal relationships between short-term phases. To process local relationships along the spatial dimension, a modified Video Swin Transformer is used as the backbone of the proposed model.

The proposed Gaze-Enhanced EgoViT model was evaluated on the EGTEA Gaze+ dataset, a comprehensive dataset for first-person view actions with gaze tracking points. The experimental results demonstrate that the proposed Gaze-Enhanced EgoViT model achieves a top-1 accuracy of 52.0\% and a top-5 accuracy of 76.3\%, representing an improvement over the baseline EgoViT model. The model was trained and tested on two versions of gaze data. Gaze version 2, with better gaze feature quality, yielded better performance, suggesting that the quality of gaze features significantly impacts the model's performance. Those results suggest that the quality of gaze features plays a significant role in the model's performance. The proposed Gaze-Enhanced EgoViT model is a promising approach to improving action recognition in egocentric videos. Additionally, experiments conducted on the model with only gaze features showed that it achieves a top-1 accuracy above 50\%, indicating that gaze information is useful for action recognition in egocentric videos. Further investigation into the impact of gaze information on \gls{ear} is warranted. 

\section{Future Works and Outlook}
\label{sec:future_work}
\textbf{Futere work:} The experimental results indicate that the proposed Gaze-Enhanced EgoViT model is a promising approach for improving action recognition performance in egocentric videos. To significantly enhance the model's performance, improving the quality of gaze features should be explored. There are three possibilities for this:
\begin{enumerate}
    \item Training and evaluating the model on a dataset with more accurate collected gaze tracking points.
    \item Studying the impact of gaze region size on \gls{ear}. Conducte experiments with different gaze-box sizes to determine the optimal size for the model.
    \item Using pretrained networks, such as autoencoders, to extract gaze features. An autoencoder can be trained to extract features from images and reconstruct the images, accuiring higher quality extracted features.
\end{enumerate}

\textbf{Outlook in the field of battery production:} The rapid growth of electric vehicles has increased the demand for batteries, making their production a key technology for value creation. As the demand and production of electric vehicles rise, so does the need for high-quality batteries. Gigafactories are being built to meet this demand, optimizing production quality, performance, and cost. Artificial intelligence technologies, such as computer vision and machine learning, are increasingly used in battery production to enhance quality. Niri et al. \cite{niri2021machine} combined machine learning to create predictive models for battery performance, linking lab-scale manufacturing to pilot-line production and supporting smarter and cleaner electrode production for high-quality Li-ion batteries. Smart factories are equipped with many industrial robotic arms, and ensuring the safety of interactions between workers and robotic arms is crucial. Suh et al. \cite{suh2023workeractivityrecognitionmanufacturing} presented a novel wearable sensing prototype integrating IMU and body capacitance sensors to recognize worker activities in manufacturing.

These studies show that artificial intelligence technologies have broad prospects in battery production. The proposed Gaze-Enhanced EgoViT model has the potential to be applied in various ways to improve production quality. For example, it could be used to monitor robotic arm activities to ensure process accuracy and avoid collisions with workers. Since the model can handle egocentric videos, the monitoring camera could be directly mounted on the moving part of the robotic arm, providing a first-person view and enhancing the environmental perception of the robotic arm. Another application could be wearable equipment for workers, such as glasses with cameras and gaze tracking functions. The proposed Gaze-Enhanced EgoViT model can recognize worker activities and identify which part of the production process the worker is engaged in, improving assembly quality by detecting anomalies in real-time. The Transformer-based model is suitable for handling varying sequence video lengths, making it ideal for real-time monitoring of different production processes performed by workers. The model has the potential to understand the relationships between different production processes and provide feedback to workers in real-time, thereby improving production efficiency and quality.