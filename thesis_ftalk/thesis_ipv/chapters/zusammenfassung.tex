This thesis investigates the integration of gaze information into a transformer-based model to enhance action recognition in egocentric videos. The study focuses on enhancing the \acrlong{dctg} module with additional gaze information.
% within EgoViT, which currently includes a \acrlong{hod} that extracts features from hand-object interactions in video frames. 
The proposed Gaze-Enhanced \gls{dctg} module fuses gaze features with hand-object features to create comprehensive gaze-hand-object features. These features generate a specialized class token that, along with the video frames, guides the transformer to focus on action-related segments. 
% Transformer-based architectures showing promising results for action recognition task in videos. 
The proposed model maintains a similar architecture to the original EgoViT, handing the temporal relationships between short-term phases effectively. A modified Video Swin Transformer serves as the backbone for processing local spatial relationships.

The EGTEA Gaze+ dataset is used for model training and testing. A series of experiments evaluates the impact of gaze information on EAR accuracy. The key objectives are to propose a novel Gaze Extrator module, which generates gaze-box image from input gaze points, and then fed them into a swquential convolutional networks to obtain the gaze features. Results demonstrate that the Gaze-Enhanced EgoViT model achieves a top-1 accuracy of 52.0\% and a top-5 accuracy of 76.3\%, surpassing the baseline EgoViT model. Higher quality gaze features significantly improve performance, with experiments showing that gaze information alone yields a top-1 accuracy above 50\%. These findings highlight the potential of the Gaze-Enhanced EgoViT model to advance action recognition in egocentric videos and suggest further exploration into the role of gaze information in enhancing EAR accuracy.


% This thesis introduces a Gaze-Enhanced \acrlong{dctg} module integrated into the EgoViT model to enhance action recognition in egocentric videos. The Gaze-Enhanced DCTG module focuses on regions of interest in video frames identified through gaze tracking points, detected hands, and objects. By incorporating gaze tracking as an additional input, the model extracts gaze features and merges them with hand-object features from a modified \acrlong{hod} module, generating dynamic class tokens that guide the model's attention. The proposed model maintains a similar architecture to the original EgoViT, handing the temporal relationships between short-term phases effectively. A modified Video Swin Transformer serves as the backbone for processing local spatial relationships. The proposed model was evaluated on the EGTEA Gaze+ dataset, which includes gaze tracking points. The proposed model's performance, notably improved with higher quality gaze features (Gaze version 2), underscores the significant impact of gaze feature quality. Experiments also indicated that the model, using only gaze features, confirming the utility of gaze information in action recognition for egocentric videos. These findings suggest the proposed Gaze-Enhanced EgoViT model as a promising approach for enhancing action recognition in egocentric videos and warrant further investigation into the impact of gaze information on \acrlong{ear}.