% !TeX spellcheck = de_DE

\chapter{Introduction}
\label{chap:introduction}
The chapter presents a comprehensive overview of the study. It begins by introducing the research background and motivation, followed by a discussion of the objectives and limitations. The chapter concludes with an outline of the remaining structure of the thesis.

\section{Background of The Study}
\label{sec:background}
Action Recognition is a computer vision task, which aims to identify and categorize the actions performed by human in video sequences. The task focus on analyzing the spatiotemporal dynamics of the actions and mapping them to predefined classes. The data is commonly collect as images and videos.

Previous researches have demonstrated significant success of neural networks in action recognition, with deep convolutional neural networks having long dominated visual modeling in computer vision \cite{he_deep_2016}, \cite{huang_densely_2018}.
However, with the advent and success of transformer, a new architecture of neural networks, many researchers now consider transformers to be a promising solution for understanding actions in images and videos.

Recent research \cite{dosovitskiy_image_2021} introduced the \gls{vit}, which achieves excellent results in image classification task using a purely transformer-based architecture. Inspired by \gls{vit}'s success in image applications, researchers explored transformer-based architectures for video recognition problems. In 2021, Arnab et al. \cite{arnab_vivit_2021} presented a pure-transformer based models for video classification. Bertasius et al. 
\cite{bertasius_is_2021} proposed TimeSformer, which adapts the standard transformer architecture to learn spatiotemporal feature of a video. Liu et al. introduced Swin Transformer and Video Swin Transformer for image and video classification using shifted Windows \cite{liu2021swin}, \cite{liu_video_2021}. These studies indicate that the Multi-Head Self-Attention mechanism in transformers not only has the ability to understand text but can also effectively interpret images and videos.

However, these studies primarily focused on standard videos. With advancements in hardware, many videos are now captured by wearable devices. Some applications in \gls{vr} and \gls{hri} require identifying actions in real-time video. These videos have different characteristics from standard videos, making action recognition more challenging. The video captured from a wearable camera provides a first-person perspective, also known as egocentric video. Unlike standard video, the camera in egocentric video is not stationary and depends on the wearerâ€™s viewpoint. It captures not only objects but also the interactions occurring between objects and subjects, causing frequent and rapid motions in the scene. This presents a challenging problem for \gls{ear}.

The hand plays a crucial role in human interactions with the world \cite{shan_understanding_2020}. Furthermore, the hand and its interactive object occupy a large percentage of the egocentric video frames. This is a significant difference  between standard and egocentric video. Therefore, recognizing the hand and the objects it interacts with is essential for \gls{ear}.

In a recent study, Pan et al. introduced the EgoViT, a model designed to consider the special properties of egocentric videos, which can seamlessly integrated with different video transformers \cite{pan_egovit_2023}. EgoViT integrates the features of hand and the object it interacts with, forcing the model to understand egocentric video by focusing on hand-object interactions. And the experiments outlined in \cite{pan_egovit_2023} demonstrate that hand-object information proves to be more valuable for EAR. The major contributions of Pan et al.'s works include the incorporation of \gls{dctg} and the \gls{padm} module into different transformers. The \gls{dctg} will detect hands and objects in video frames, then the extracted hand-object features will be used as class tokens sent to transformers. This module forces the transformer to focus on specific features from the original video. Since \gls{dctg} can generate the class token from given features, and as mentioned in \cite{pan_egovit_2023}, it has the potential to process features from videos beyond just hand-object interactions. Future studies could explore how other features impact the accuracy of action recognition.

A possible additional feature in egocentric videos is gaze information. Understanding visual attention is significantly valuable across various applications. A growing number of devices in applications like \gls{vr} and \gls{hri} are capable to record the gaze data from user. Research by Hayhoe et al. \cite{hayhoe_eye_2005} indicated eye movements are crucial for understanding human intention in daily activities. Another study by Land et al. \cite{land_roles_1999} demonstrated that in object-related actions, the direction of gaze is closely connected with the specific act. These studies provide a theoretical basis for the importance of visual information in egocentric videos. This perspective suggests that integrating gaze information has the potential to enhance the accuracy of  \gls{ear}.

Furthermore, several studies have examined gaze information in egocentric video. Huang et al. \cite{huang_predicting_2018} developed a computational model for predicting the camera wearer's point-of-gaze from egocentric video. In \cite{tavakoli_digging_2019} explored bottom-up and top-down attentional cues involved in guiding first-person gaze. Lai et al. \cite{lai_eye_2022} introduced a transformer based model that calculates spatiotemporal global-local correlation for egocentric gaze estimation. Despite interest in gaze information in egocentric video, none of these studies have used gaze data collected from capture devices in their models.

The commonly used large-scale dataset for human action recognition in standard videos include Kinetics-400 and Something-Something v2. EPIC-KITCHENS-100 is a widely used dataset for egocentric videos, but the datasets mentioned above lake of gaze data. Several newly collected first-person view datasets, such as GTEA Gaze+ \cite{li_eye_2020} and Ego4D \cite{grauman_ego4d_2022}, including the gaze information at the frame level. The gaze point of the person is recorded simultaneously during video capture.

Although many studies have hinted at the impotence of gaze information in \gls{ear}, there is a limited body of research that specifically addresses gaze information within transformer based models. As highlighted in \cite{pan_egovit_2023}, EgoViT, specifically the \gls{dctg} module, has the potential to integrate additional information, including gaze data.

In this thesis, the gaze points collected from the dataset will be utilized to explore their impact on the accuracy of \gls{ear} in a transformer-based model. The proposed model will build upon the hierarchical architecture of EgoViT, using Video Swin Transformer as the backbone. The EGTEA Gaze+ dataset will be employed for this study.
\clearpage

\section{Objectives}
\label{sec:objectives}
The goal of this thesis is to study the impact of additional gaze information on a transformer-based model in \gls{ear}. The architecture of EgoViT is specifically designed for egocentric video, with its \gls{padm} module could effectively be processing the sequential phases in the video. With its ability to better understand rapid scene changes in videos, EgoViT has significant potential for this task. Therefore, the proposed model in this thesis is base on the EgoViT framework.

The key component of EgoViT is the \gls{dctg} module, which includes a \gls{hod}. The \gls{hod} extracts hand-object features from the detected hand-object parts in a frame. This thesis aims to enhance the \gls{dctg} module by integrating additional gaze information. The original \gls{dctg} in EgoViT will be extended to fuse gaze information, processed from gaze points, with hand-object information from \gls{hod}. By combining gaze, hand, and object data, the model will extract gaze-hand-object features. Finally, a class token will be generated from these gaze-hand-object features. This special class token, along with the video frames, will be sent to the subsequent layers in the transformer. This process helps the transformer-based model concentrate on the action-related parts of the video.

EGTEA Gaze+ dataset is used for model training and testing. A series of experiments will be conducted to study how gaze information affect the accuracy of \gls{ear}. Consequently, the objectives of this study are as follows:
\begin{itemize}
    \item Propose a transformer-based method for \gls{ear}, notable for its novel incorporation of gaze information. The model incorporates both hand-object interactions and gaze information simultaneously.
    \item Enhanced the \gls{dctg} with additional gaze information. The Gaze-Enhanced \gls{dctg} module include a gaze-box cropper and sequential convolutinal networks to extract gaze features. This enhancement will guide the transformer to concentrate more effectively on the most informative segments of the video.
    \item Explore how to combine the class token and normal tokens in the transformer to improve the results of \gls{ear}.
\end{itemize}



% \begin{enumerate}
%     \item Proposal of a transformer-based method for \gls{ear}, notable for its novel incorporation of gaze information. This method, developed on the EgoViT framework, will incorporate both hand-object interactions and gaze information simultaneously, offering a more comprehensive approach to understanding egocentric video data.
%     \item Development of a new \gls{dctg} model, designed to improve the integration of gaze data.
%     This enhancement guides the transformer to concentrate more effectively on the most informative segments of the video.
% \end{enumerate}
\clearpage
\section{Thesis Structure}
\label{sec:thesis structure}
With the completion of the first chapter, the following paragraphs provide a comprehensive overview of the subsequent chapters.
% Before delving into specifics, the following paragraphs provide a brief summary of this thesis and its organization:

\textbf{Chapter 2: Related Works} reviews the existing literature and studies related to the research topic, providing an overview of transformer-based model in egocentric video.

\textbf{Chapter 3: Methodology} outlines the structure of the proposed model and the procedures employed in the study, detailing the approach, data collection, and key component explanations.

\textbf{Chapter 4: Experiments and Results} describes the practical implementation of the proposed model, including the experiments and results obtained from the EGTEA Gaze+ dataset. At end of this chapter, the results will be discussed.

\textbf{Chapter 5: Summery} summarizes the study, highlighting the key findings and contributions. This chapter also discusses the potential applications of the proposed model in battery production.