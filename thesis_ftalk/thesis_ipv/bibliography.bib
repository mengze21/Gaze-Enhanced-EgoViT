% Encoding: UTF-8

%%%
%
%Beim Erstellen der Bibtex-Datei wird empfohlen darauf zu achten, dass die DOI aufgeführt wird.
%
%%%

@book{WSPA,
author={Sanjiva Weerawarana and Francisco Curbera and Frank Leymann and Tony Storey and Donald F. Ferguson},
title={Web Services Platform Architecture : SOAP, WSDL, WS-Policy, WS-Addressing, WS-BPEL, WS-Reliable Messaging, and More},
year={2005},
price={$31.49},
publisher={Prentice Hall PTR},
isbn={0131488740},
doi = {10.1.1/jpb001}
}


% !! DO NOT USE `label =   {ASF}` in other Misc entries !!

@Misc{ApacheODE,
  author = {{The Apache Software Foundation}},
  title =  {Apache ODE\texttrademark{} -- The Orchestration Director Engine},
  year =   {2016},
  label =  {ASF},
  url =    {http://ode.apache.org}
}

@Article{RVvdA2016,
  author =    {H.A. Reijers and I. Vanderfeesten and W.M.P. van der Aalst},
  title =     {The effectiveness of workflow management systems: A longitudinal study},
  journal =   {International Journal of Information Management},
  year =      {2016},
  volume =    {36},
  number =    {1},
  pages =     {126--141},
  month =     feb,
  doi =       {10.1016/j.ijinfomgt.2015.08.003},
  publisher = {Elsevier {BV}}
}

@Comment{jabref-meta: databaseType:biblatex;}

@misc{goldschmid_reinforcement_2023,
	title = {Reinforcement {Learning} based {Autonomous} {Multi}-{Rotor} {Landing} on {Moving} {Platforms}},
	url = {http://arxiv.org/abs/2302.13192},
	abstract = {Multi-rotor UAVs suffer from a restricted range and flight duration due to limited battery capacity. Autonomous landing on a 2D moving platform offers the possibility to replenish batteries and offload data, thus increasing the utility of the vehicle. Classical approaches rely on accurate, complex and difficult-to-derive models of the vehicle and the environment. Reinforcement learning (RL) provides an attractive alternative due to its ability to learn a suitable control policy exclusively from data during a training procedure. However, current methods require several hours to train, have limited success rates and depend on hyperparameters that need to be tuned by trial-and-error. We address all these issues in this work. First, we decompose the landing procedure into a sequence of simpler, but similar learning tasks. This is enabled by applying two instances of the same RL based controller trained for 1D motion for controlling the multi-rotor's movement in both the longitudinal and the lateral direction. Second, we introduce a powerful state space discretization technique that is based on i) kinematic modeling of the moving platform to derive information about the state space topology and ii) structuring the training as a sequential curriculum using transfer learning. Third, we leverage the kinematics model of the moving platform to also derive interpretable hyperparameters for the training process that ensure sufficient maneuverability of the multi-rotor vehicle. The training is performed using the tabular RL method Double Q-Learning. Through extensive simulations we show that the presented method significantly increases the rate of successful landings, while requiring less training time compared to other deep RL approaches. Finally, we deploy and demonstrate our algorithm on real hardware. For all evaluation scenarios we provide statistics on the agent's performance.},
	
	publisher = {arXiv},
	author = {Goldschmid, Pascal and Ahmad, Aamir},
	month = feb,
	year = {2023},
	note = {arXiv:2302.13192 [cs, eess]},
	keywords = {Computer Science - Robotics, Electrical Engineering and Systems Science - Systems and Control},
	file = {arXiv Fulltext PDF:/Users/mengze/Zotero/storage/8ZF37C9N/Goldschmid 和 Ahmad - 2023 - Reinforcement Learning based Autonomous Multi-Roto.pdf:application/pdf;arXiv.org Snapshot:/Users/mengze/Zotero/storage/ZY39GC8I/2302.html:text/html},
}

@misc{qin_geometric_2022,
	title = {Geometric {Transformer} for {Fast} and {Robust} {Point} {Cloud} {Registration}},
	url = {http://arxiv.org/abs/2202.06688},
	abstract = {We study the problem of extracting accurate correspondences for point cloud registration. Recent keypoint-free methods bypass the detection of repeatable keypoints which is difficult in low-overlap scenarios, showing great potential in registration. They seek correspondences over downsampled superpoints, which are then propagated to dense points. Superpoints are matched based on whether their neighboring patches overlap. Such sparse and loose matching requires contextual features capturing the geometric structure of the point clouds. We propose Geometric Transformer to learn geometric feature for robust superpoint matching. It encodes pair-wise distances and triplet-wise angles, making it robust in low-overlap cases and invariant to rigid transformation. The simplistic design attains surprisingly high matching accuracy such that no RANSAC is required in the estimation of alignment transformation, leading to \$100\$ times acceleration. Our method improves the inlier ratio by \$17\{{\textbackslash}sim\}30\$ percentage points and the registration recall by over \$7\$ points on the challenging 3DLoMatch benchmark. Our code and models are available at https://github.com/qinzheng93/GeoTransformer.},
	
	publisher = {arXiv},
	author = {Qin, Zheng and Yu, Hao and Wang, Changjian and Guo, Yulan and Peng, Yuxing and Xu, Kai},
	month = mar,
	year = {2022},
	note = {arXiv:2202.06688 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/mengze/Zotero/storage/DCJ6KZGR/Qin 等 - 2022 - Geometric Transformer for Fast and Robust Point Cl.pdf:application/pdf;arXiv.org Snapshot:/Users/mengze/Zotero/storage/5HG7SP4D/2202.html:text/html},
}

@misc{pan_egovit_2023,
	title = {{EgoViT}: {Pyramid} {Video} {Transformer} for {Egocentric} {Action} {Recognition}},
	shorttitle = {{EgoViT}},
	url = {http://arxiv.org/abs/2303.08920},
	abstract = {Capturing interaction of hands with objects is important to autonomously detect human actions from egocentric videos. In this work, we present a pyramid video transformer with a dynamic class token generator for egocentric action recognition. Different from previous video transformers, which use the same static embedding as the class token for diverse inputs, we propose a dynamic class token generator that produces a class token for each input video by analyzing the hand-object interaction and the related motion information. The dynamic class token can diffuse such information to the entire model by communicating with other informative tokens in the subsequent transformer layers. With the dynamic class token, dissimilarity between videos can be more prominent, which helps the model distinguish various inputs. In addition, traditional video transformers explore temporal features globally, which requires large amounts of computation. However, egocentric videos often have a large amount of background scene transition, which causes discontinuities across distant frames. In this case, blindly reducing the temporal sampling rate will risk losing crucial information. Hence, we also propose a pyramid architecture to hierarchically process the video from short-term high rate to long-term low rate. With the proposed architecture, we significantly reduce the computational cost as well as the memory requirement without sacrificing from the model performance. We perform comparisons with different baseline video transformers on the EPIC-KITCHENS-100 and EGTEA Gaze+ datasets. Both quantitative and qualitative results show that the proposed model can efficiently improve the performance for egocentric action recognition.},
	
	publisher = {arXiv},
	author = {Pan, Chenbin and Zhang, Zhiqi and Velipasalar, Senem and Xu, Yi},
	month = mar,
	year = {2023},
	note = {arXiv:2303.08920 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/mengze/Zotero/storage/2HMEFC85/Pan et al. - 2023 - EgoViT Pyramid Video Transformer for Egocentric A.pdf:application/pdf;arXiv.org Snapshot:/Users/mengze/Zotero/storage/LAYB74H2/2303.html:text/html;Full Text:/Users/mengze/Zotero/storage/DBYF6YLY/Pan et al. - 2023 - EgoViT Pyramid Video Transformer for Egocentric A.pdf:application/pdf},
}


@misc{lai_eye_2022,
	title = {In the {Eye} of {Transformer}: {Global}-{Local} {Correlation} for {Egocentric} {Gaze} {Estimation}},
	shorttitle = {In the {Eye} of {Transformer}},
	url = {http://arxiv.org/abs/2208.04464},
	abstract = {In this paper, we present the first transformer-based model to address the challenging problem of egocentric gaze estimation. We observe that the connection between the global scene context and local visual information is vital for localizing the gaze fixation from egocentric video frames. To this end, we design the transformer encoder to embed the global context as one additional visual token and further propose a novel Global-Local Correlation (GLC) module to explicitly model the correlation of the global token and each local token. We validate our model on two egocentric video datasets - EGTEA Gaze+ and Ego4D. Our detailed ablation studies demonstrate the benefits of our method. In addition, our approach exceeds previous state-of-the-arts by a large margin. We also provide additional visualizations to support our claim that global-local correlation serves a key representation for predicting gaze fixation from egocentric videos. More details can be found in our website (https://bolinlai.github.io/GLC-EgoGazeEst).},

	publisher = {arXiv},
	author = {Lai, Bolin and Liu, Miao and Ryan, Fiona and Rehg, James M.},
	month = aug,
	year = {2022},
	note = {arXiv:2208.04464 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/mengze/Zotero/storage/4TNKX5JQ/Lai et al. - 2022 - In the Eye of Transformer Global-Local Correlatio.pdf:application/pdf;arXiv.org Snapshot:/Users/mengze/Zotero/storage/66ZSRTXD/2208.html:text/html},
}

@inproceedings{shvetsova_everything_2022,
	address = {New Orleans, LA, USA},
	title = {Everything at {Once} – {Multi}-modal {Fusion} {Transformer} for {Video} {Retrieval}},
	isbn = {978-1-66546-946-3},
	url = {https://ieeexplore.ieee.org/document/9879495/},
	doi = {10.1109/CVPR52688.2022.01939},
	language = {en},
	
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Shvetsova, Nina and Chen, Brian and Rouditchenko, Andrew and Thomas, Samuel and Kingsbury, Brian and Feris, Rogerio and Harwath, David and Glass, James and Kuehne, Hilde},
	month = jun,
	year = {2022},
	pages = {19988--19997},
	file = {Shvetsova et al. - 2022 - Everything at Once – Multi-modal Fusion Transforme.pdf:/Users/mengze/Zotero/storage/Q2TBJQLX/Shvetsova et al. - 2022 - Everything at Once – Multi-modal Fusion Transforme.pdf:application/pdf},
}

@misc{bertasius_is_2021,
	title = {Is {Space}-{Time} {Attention} {All} {You} {Need} for {Video} {Understanding}?},
	url = {http://arxiv.org/abs/2102.05095},
	abstract = {We present a convolution-free approach to video classification built exclusively on self-attention over space and time. Our method, named "TimeSformer," adapts the standard Transformer architecture to video by enabling spatiotemporal feature learning directly from a sequence of frame-level patches. Our experimental study compares different self-attention schemes and suggests that "divided attention," where temporal attention and spatial attention are separately applied within each block, leads to the best video classification accuracy among the design choices considered. Despite the radically new design, TimeSformer achieves state-of-the-art results on several action recognition benchmarks, including the best reported accuracy on Kinetics-400 and Kinetics-600. Finally, compared to 3D convolutional networks, our model is faster to train, it can achieve dramatically higher test efficiency (at a small drop in accuracy), and it can also be applied to much longer video clips (over one minute long). Code and models are available at: https://github.com/facebookresearch/TimeSformer.},
	
	publisher = {arXiv},
	author = {Bertasius, Gedas and Wang, Heng and Torresani, Lorenzo},
	month = jun,
	year = {2021},
	note = {arXiv:2102.05095 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/mengze/Zotero/storage/23INWZ9U/Bertasius et al. - 2021 - Is Space-Time Attention All You Need for Video Und.pdf:application/pdf;arXiv.org Snapshot:/Users/mengze/Zotero/storage/TEBFV9MU/2102.html:text/html},
}

@inproceedings{strudel_segmenter_2021,
	address = {Montreal, QC, Canada},
	title = {Segmenter: {Transformer} for {Semantic} {Segmentation}},
	isbn = {978-1-66542-812-5},
	shorttitle = {Segmenter},
	url = {https://ieeexplore.ieee.org/document/9710959/},
	doi = {10.1109/ICCV48922.2021.00717},
	
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Strudel, Robin and Garcia, Ricardo and Laptev, Ivan and Schmid, Cordelia},
	month = oct,
	year = {2021},
	pages = {7242--7252},
	file = {Submitted Version:/Users/mengze/Zotero/storage/25UQBJYG/Strudel et al. - 2021 - Segmenter Transformer for Semantic Segmentation.pdf:application/pdf},
}

@article{huang_ego-vision_2020,
	title = {An {Ego}-{Vision} {System} for {Discovering} {Human} {Joint} {Attention}},
	volume = {50},
	issn = {2168-2291, 2168-2305},
	url = {https://ieeexplore.ieee.org/document/9057439/},
	doi = {10.1109/THMS.2020.2965429},
	number = {4},
	
	journal = {IEEE Transactions on Human-Machine Systems},
	author = {Huang, Yifei and Cai, Minjie and Sato, Yoichi},
	month = aug,
	year = {2020},
	pages = {306--316},
	file = {An_Ego-Vision_System_for_Discovering_Human_Joint_Attention.pdf:/Users/mengze/Documents/文稿 - Mengze的MacBook Pro/paper/An_Ego-Vision_System_for_Discovering_Human_Joint_Attention.pdf:application/pdf},
}

@misc{vaswani_attention_2023,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = aug,
	year = {2023},
	note = {arXiv:1706.03762 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/mengze/Zotero/storage/H8JYYZNA/Vaswani et al. - 2023 - Attention Is All You Need.pdf:application/pdf;arXiv.org Snapshot:/Users/mengze/Zotero/storage/73P9HBQI/1706.html:text/html},
}

@misc{dosovitskiy_image_2021,
	title = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
	shorttitle = {An {Image} is {Worth} 16x16 {Words}},
	url = {http://arxiv.org/abs/2010.11929},
	abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
	
	publisher = {arXiv},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	month = jun,
	year = {2021},
	note = {arXiv:2010.11929 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/mengze/Zotero/storage/MQ5BGUSJ/Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Im.pdf:application/pdf;arXiv.org Snapshot:/Users/mengze/Zotero/storage/SGKWQP5D/2010.html:text/html},
}

@misc{arnab_vivit_2021,
	title = {{ViViT}: {A} {Video} {Vision} {Transformer}},
	shorttitle = {{ViViT}},
	url = {http://arxiv.org/abs/2103.15691},
	abstract = {We present pure-transformer based models for video classification, drawing upon the recent success of such models in image classification. Our model extracts spatio-temporal tokens from the input video, which are then encoded by a series of transformer layers. In order to handle the long sequences of tokens encountered in video, we propose several, efficient variants of our model which factorise the spatial- and temporal-dimensions of the input. Although transformer-based models are known to only be effective when large training datasets are available, we show how we can effectively regularise the model during training and leverage pretrained image models to be able to train on comparatively small datasets. We conduct thorough ablation studies, and achieve state-of-the-art results on multiple video classification benchmarks including Kinetics 400 and 600, Epic Kitchens, Something-Something v2 and Moments in Time, outperforming prior methods based on deep 3D convolutional networks. To facilitate further research, we release code at https://github.com/google-research/scenic/tree/main/scenic/projects/vivit},
	
	publisher = {arXiv},
	author = {Arnab, Anurag and Dehghani, Mostafa and Heigold, Georg and Sun, Chen and Lučić, Mario and Schmid, Cordelia},
	month = nov,
	year = {2021},
	note = {arXiv:2103.15691 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/mengze/Zotero/storage/YHUSN4MZ/Arnab et al. - 2021 - ViViT A Video Vision Transformer.pdf:application/pdf;arXiv.org Snapshot:/Users/mengze/Zotero/storage/N5TECX65/2103.html:text/html},
}

@misc{liu_video_2021,
	title = {Video {Swin} {Transformer}},
	url = {http://arxiv.org/abs/2106.13230},
	abstract = {The vision community is witnessing a modeling shift from CNNs to Transformers, where pure Transformer architectures have attained top accuracy on the major video recognition benchmarks. These video models are all built on Transformer layers that globally connect patches across the spatial and temporal dimensions. In this paper, we instead advocate an inductive bias of locality in video Transformers, which leads to a better speed-accuracy trade-off compared to previous approaches which compute self-attention globally even with spatial-temporal factorization. The locality of the proposed video architecture is realized by adapting the Swin Transformer designed for the image domain, while continuing to leverage the power of pre-trained image models. Our approach achieves state-of-the-art accuracy on a broad range of video recognition benchmarks, including on action recognition (84.9 top-1 accuracy on Kinetics-400 and 86.1 top-1 accuracy on Kinetics-600 with {\textasciitilde}20x less pre-training data and {\textasciitilde}3x smaller model size) and temporal modeling (69.6 top-1 accuracy on Something-Something v2). The code and models will be made publicly available at https://github.com/SwinTransformer/Video-Swin-Transformer.},
	
	publisher = {arXiv},
	author = {Liu, Ze and Ning, Jia and Cao, Yue and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Hu, Han},
	month = jun,
	year = {2021},
	note = {arXiv:2106.13230 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/mengze/Zotero/storage/2GNTS8CG/Liu et al. - 2021 - Video Swin Transformer.pdf:application/pdf;arXiv.org Snapshot:/Users/mengze/Zotero/storage/BVPHYLTK/2106.html:text/html},
}

@misc{herzig_object-region_2022,
	title = {Object-{Region} {Video} {Transformers}},
	url = {http://arxiv.org/abs/2110.06915},
	abstract = {Recently, video transformers have shown great success in video understanding, exceeding CNN performance; yet existing video transformer models do not explicitly model objects, although objects can be essential for recognizing actions. In this work, we present Object-Region Video Transformers (ORViT), an {\textbackslash}emph\{object-centric\} approach that extends video transformer layers with a block that directly incorporates object representations. The key idea is to fuse object-centric representations starting from early layers and propagate them into the transformer-layers, thus affecting the spatio-temporal representations throughout the network. Our ORViT block consists of two object-level streams: appearance and dynamics. In the appearance stream, an "Object-Region Attention" module applies self-attention over the patches and {\textbackslash}emph\{object regions\}. In this way, visual object regions interact with uniform patch tokens and enrich them with contextualized object information. We further model object dynamics via a separate "Object-Dynamics Module", which captures trajectory interactions, and show how to integrate the two streams. We evaluate our model on four tasks and five datasets: compositional and few-shot action recognition on SomethingElse, spatio-temporal action detection on AVA, and standard action recognition on Something-Something V2, Diving48 and Epic-Kitchen100. We show strong performance improvement across all tasks and datasets considered, demonstrating the value of a model that incorporates object representations into a transformer architecture. For code and pretrained models, visit the project page at {\textbackslash}url\{https://roeiherz.github.io/ORViT/\}},
	
	publisher = {arXiv},
	author = {Herzig, Roei and Ben-Avraham, Elad and Mangalam, Karttikeya and Bar, Amir and Chechik, Gal and Rohrbach, Anna and Darrell, Trevor and Globerson, Amir},
	month = jun,
	year = {2022},
	note = {arXiv:2110.06915 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/mengze/Zotero/storage/ETMLBYV8/Herzig et al. - 2022 - Object-Region Video Transformers.pdf:application/pdf;arXiv.org Snapshot:/Users/mengze/Zotero/storage/C8ZQ6FLF/2110.html:text/html},
}

@inproceedings{shan_understanding_2020,
	address = {Seattle, WA, USA},
	title = {Understanding {Human} {Hands} in {Contact} at {Internet} {Scale}},
	isbn = {978-1-72817-168-5},
	url = {https://ieeexplore.ieee.org/document/9157473/},
	doi = {10.1109/CVPR42600.2020.00989},
	
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Shan, Dandan and Geng, Jiaqi and Shu, Michelle and Fouhey, David F.},
	month = jun,
	year = {2020},
	pages = {9866--9875},
	file = {Submitted Version:/Users/mengze/Zotero/storage/5D3G97NL/Shan et al. - 2020 - Understanding Human Hands in Contact at Internet S.pdf:application/pdf},
}

@misc{hou_visual_2020,
	title = {Visual {Compositional} {Learning} for {Human}-{Object} {Interaction} {Detection}},
	url = {http://arxiv.org/abs/2007.12407},
	abstract = {Human-Object interaction (HOI) detection aims to localize and infer relationships between human and objects in an image. It is challenging because an enormous number of possible combinations of objects and verbs types forms a long-tail distribution. We devise a deep Visual Compositional Learning (VCL) framework, which is a simple yet efficient framework to effectively address this problem. VCL first decomposes an HOI representation into object and verb specific features, and then composes new interaction samples in the feature space via stitching the decomposed features. The integration of decomposition and composition enables VCL to share object and verb features among different HOI samples and images, and to generate new interaction samples and new types of HOI, and thus largely alleviates the long-tail distribution problem and benefits low-shot or zero-shot HOI detection. Extensive experiments demonstrate that the proposed VCL can effectively improve the generalization of HOI detection on HICO-DET and V-COCO and outperforms the recent state-of-the-art methods on HICO-DET. Code is available at https://github.com/zhihou7/VCL.},
	
	publisher = {arXiv},
	author = {Hou, Zhi and Peng, Xiaojiang and Qiao, Yu and Tao, Dacheng},
	month = oct,
	year = {2020},
	note = {arXiv:2007.12407 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/mengze/Zotero/storage/JFR9UYCL/Hou et al. - 2020 - Visual Compositional Learning for Human-Object Int.pdf:application/pdf;arXiv.org Snapshot:/Users/mengze/Zotero/storage/I8V3BD53/2007.html:text/html},
}

@article{damen_epic-kitchens_2021,
	title = {The {EPIC}-{KITCHENS} {Dataset}: {Collection}, {Challenges} and {Baselines}},
	volume = {43},
	issn = {0162-8828, 2160-9292, 1939-3539},
	shorttitle = {The {EPIC}-{KITCHENS} {Dataset}},
	url = {https://ieeexplore.ieee.org/document/9084270/},
	doi = {10.1109/TPAMI.2020.2991965},
	number = {11},
	
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Damen, Dima and Doughty, Hazel and Farinella, Giovanni Maria and Fidler, Sanja and Furnari, Antonino and Kazakos, Evangelos and Moltisanti, Davide and Munro, Jonathan and Perrett, Toby and Price, Will and Wray, Michael},
	month = nov,
	year = {2021},
	pages = {4125--4141},
	file = {Submitted Version:/Users/mengze/Zotero/storage/GKFB935M/Damen et al. - 2021 - The EPIC-KITCHENS Dataset Collection, Challenges .pdf:application/pdf},
}

@misc{sigurdsson_charades-ego_2018,
	title = {Charades-{Ego}: {A} {Large}-{Scale} {Dataset} of {Paired} {Third} and {First} {Person} {Videos}},
	shorttitle = {Charades-{Ego}},
	url = {http://arxiv.org/abs/1804.09626},
	abstract = {In Actor and Observer we introduced a dataset linking the first and third-person video understanding domains, the Charades-Ego Dataset. In this paper we describe the egocentric aspect of the dataset and present annotations for Charades-Ego with 68,536 activity instances in 68.8 hours of first and third-person video, making it one of the largest and most diverse egocentric datasets available. Charades-Ego furthermore shares activity classes, scripts, and methodology with the Charades dataset, that consist of additional 82.3 hours of third-person video with 66,500 activity instances. Charades-Ego has temporal annotations and textual descriptions, making it suitable for egocentric video classification, localization, captioning, and new tasks utilizing the cross-modal nature of the data.},
	
	publisher = {arXiv},
	author = {Sigurdsson, Gunnar A. and Gupta, Abhinav and Schmid, Cordelia and Farhadi, Ali and Alahari, Karteek},
	month = apr,
	year = {2018},
	note = {arXiv:1804.09626 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/mengze/Zotero/storage/5HIH8SHC/Sigurdsson et al. - 2018 - Charades-Ego A Large-Scale Dataset of Paired Thir.pdf:application/pdf;arXiv.org Snapshot:/Users/mengze/Zotero/storage/6TK2RIKE/1804.html:text/html},
}

@article{wang_symbiotic_2020,
	title = {Symbiotic {Attention} with {Privileged} {Information} for {Egocentric} {Action} {Recognition}},
	volume = {34},
	issn = {2374-3468, 2159-5399},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/6907},
	doi = {10.1609/aaai.v34i07.6907},
	abstract = {Egocentric video recognition is a natural testbed for diverse interaction reasoning. Due to the large action vocabulary in egocentric video datasets, recent studies usually utilize a two-branch structure for action recognition, i.e., one branch for verb classification and the other branch for noun classification. However, correlation study between the verb and the noun branches have been largely ignored. Besides, the two branches fail to exploit local features due to the absence of position-aware attention mechanism. In this paper, we propose a novel Symbiotic Attention framework leveraging Privileged information (SAP) for egocentric video recognition. Finer position-aware object detection features can facilitate the understanding of actor's interaction with the object. We introduce these features in action recognition and regard them as privileged information. Our framework enables mutual communication among the verb branch, the noun branch, and the privileged information. This communication process not only injects local details into global features, but also exploits implicit guidance about the spatio-temporal position of an on-going action. We introduce a novel symbiotic attention (SA) to enable effective communication. It first normalizes the detection guided features on one branch to underline the action-relevant information from the other branch. SA adaptively enhances the interactions among the three sources. To further catalyze this communication, spatial relations are uncovered for the selection of most action-relevant information. It identifies the most valuable and discriminative feature for classification. We validate the effectiveness of our SAP quantitatively and qualitatively. Notably, it achieves the state-of-the-art on two large-scale egocentric video datasets.},
	number = {07},
	
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Wang, Xiaohan and Wu, Yu and Zhu, Linchao and Yang, Yi},
	month = apr,
	year = {2020},
	pages = {12249--12256},
	file = {Full Text:/Users/mengze/Zotero/storage/9GFU84CD/Wang et al. - 2020 - Symbiotic Attention with Privileged Information fo.pdf:application/pdf},
}

@misc{huang_towards_2021,
	title = {Towards {Training} {Stronger} {Video} {Vision} {Transformers} for {EPIC}-{KITCHENS}-100 {Action} {Recognition}},
	url = {http://arxiv.org/abs/2106.05058},
	abstract = {With the recent surge in the research of vision transformers, they have demonstrated remarkable potential for various challenging computer vision applications, such as image recognition, point cloud classification as well as video understanding. In this paper, we present empirical results for training a stronger video vision transformer on the EPIC-KITCHENS-100 Action Recognition dataset. Specifically, we explore training techniques for video vision transformers, such as augmentations, resolutions as well as initialization, etc. With our training recipe, a single ViViT model achieves the performance of 47.4{\textbackslash}\% on the validation set of EPIC-KITCHENS-100 dataset, outperforming what is reported in the original paper by 3.4\%. We found that video transformers are especially good at predicting the noun in the verb-noun action prediction task. This makes the overall action prediction accuracy of video transformers notably higher than convolutional ones. Surprisingly, even the best video transformers underperform the convolutional networks on the verb prediction. Therefore, we combine the video vision transformers and some of the convolutional video networks and present our solution to the EPIC-KITCHENS-100 Action Recognition competition.},
	
	publisher = {arXiv},
	author = {Huang, Ziyuan and Qing, Zhiwu and Wang, Xiang and Feng, Yutong and Zhang, Shiwei and Jiang, Jianwen and Xia, Zhurong and Tang, Mingqian and Sang, Nong and Ang Jr, Marcelo H.},
	month = jun,
	year = {2021},
	note = {arXiv:2106.05058 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/mengze/Zotero/storage/R6DC825B/Huang et al. - 2021 - Towards Training Stronger Video Vision Transformer.pdf:application/pdf;arXiv.org Snapshot:/Users/mengze/Zotero/storage/RG9UGFAT/2106.html:text/html},
}

@article{hayhoe_eye_2005,
	title = {Eye movements in natural behavior},
	volume = {9},
	issn = {13646613},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661305000598},
	doi = {10.1016/j.tics.2005.02.009},
	language = {en},
	number = {4},
	
	journal = {Trends in Cognitive Sciences},
	author = {Hayhoe, Mary and Ballard, Dana},
	month = apr,
	year = {2005},
	pages = {188--194},
	file = {Hayhoe and Ballard - 2005 - Eye movements in natural behavior.pdf:/Users/mengze/Zotero/storage/S6IJRPY7/Hayhoe and Ballard - 2005 - Eye movements in natural behavior.pdf:application/pdf},
}

@inproceedings{he_deep_2016,
	address = {Las Vegas, NV, USA},
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780459/},
	doi = {10.1109/CVPR.2016.90},
	
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = jun,
	year = {2016},
	pages = {770--778},
	file = {Accepted Version:/Users/mengze/Zotero/storage/53RUHC6D/He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf:application/pdf},
}

@misc{huang_densely_2018,
	title = {Densely {Connected} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1608.06993},
	abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet .},
	
	publisher = {arXiv},
	author = {Huang, Gao and Liu, Zhuang and van der Maaten, Laurens and Weinberger, Kilian Q.},
	month = jan,
	year = {2018},
	note = {arXiv:1608.06993 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/mengze/Zotero/storage/VLYE62D4/Huang et al. - 2018 - Densely Connected Convolutional Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/mengze/Zotero/storage/KXBSBP5X/1608.html:text/html},
}

@article{li_trear_2022,
	title = {Trear: {Transformer}-{Based} {RGB}-{D} {Egocentric} {Action} {Recognition}},
	volume = {14},
	issn = {2379-8920, 2379-8939},
	shorttitle = {Trear},
	url = {https://ieeexplore.ieee.org/document/9312201/},
	doi = {10.1109/TCDS.2020.3048883},
	number = {1},
	
	journal = {IEEE Transactions on Cognitive and Developmental Systems},
	author = {Li, Xiangyu and Hou, Yonghong and Wang, Pichao and Gao, Zhimin and Xu, Mingliang and Li, Wanqing},
	month = mar,
	year = {2022},
	pages = {246--252},
	file = {Submitted Version:/Users/mengze/Zotero/storage/LP6DST9L/Li et al. - 2022 - Trear Transformer-Based RGB-D Egocentric Action R.pdf:application/pdf},
}

@misc{grauman_ego4d_2022,
	title = {{Ego4D}: {Around} the {World} in 3,000 {Hours} of {Egocentric} {Video}},
	shorttitle = {{Ego4D}},
	url = {http://arxiv.org/abs/2110.07058},
	abstract = {We introduce Ego4D, a massive-scale egocentric video dataset and benchmark suite. It offers 3,670 hours of daily-life activity video spanning hundreds of scenarios (household, outdoor, workplace, leisure, etc.) captured by 931 unique camera wearers from 74 worldwide locations and 9 different countries. The approach to collection is designed to uphold rigorous privacy and ethics standards with consenting participants and robust de-identification procedures where relevant. Ego4D dramatically expands the volume of diverse egocentric video footage publicly available to the research community. Portions of the video are accompanied by audio, 3D meshes of the environment, eye gaze, stereo, and/or synchronized videos from multiple egocentric cameras at the same event. Furthermore, we present a host of new benchmark challenges centered around understanding the first-person visual experience in the past (querying an episodic memory), present (analyzing hand-object manipulation, audio-visual conversation, and social interactions), and future (forecasting activities). By publicly sharing this massive annotated dataset and benchmark suite, we aim to push the frontier of first-person perception. Project page: https://ego4d-data.org/},
	
	publisher = {arXiv},
	author = {Grauman, Kristen and Westbury, Andrew and Byrne, Eugene and Chavis, Zachary and Furnari, Antonino and Girdhar, Rohit and Hamburger, Jackson and Jiang, Hao and Liu, Miao and Liu, Xingyu and Martin, Miguel and Nagarajan, Tushar and Radosavovic, Ilija and Ramakrishnan, Santhosh Kumar and Ryan, Fiona and Sharma, Jayant and Wray, Michael and Xu, Mengmeng and Xu, Eric Zhongcong and Zhao, Chen and Bansal, Siddhant and Batra, Dhruv and Cartillier, Vincent and Crane, Sean and Do, Tien and Doulaty, Morrie and Erapalli, Akshay and Feichtenhofer, Christoph and Fragomeni, Adriano and Fu, Qichen and Gebreselasie, Abrham and Gonzalez, Cristina and Hillis, James and Huang, Xuhua and Huang, Yifei and Jia, Wenqi and Khoo, Weslie and Kolar, Jachym and Kottur, Satwik and Kumar, Anurag and Landini, Federico and Li, Chao and Li, Yanghao and Li, Zhenqiang and Mangalam, Karttikeya and Modhugu, Raghava and Munro, Jonathan and Murrell, Tullie and Nishiyasu, Takumi and Price, Will and Puentes, Paola Ruiz and Ramazanova, Merey and Sari, Leda and Somasundaram, Kiran and Southerland, Audrey and Sugano, Yusuke and Tao, Ruijie and Vo, Minh and Wang, Yuchen and Wu, Xindi and Yagi, Takuma and Zhao, Ziwei and Zhu, Yunyi and Arbelaez, Pablo and Crandall, David and Damen, Dima and Farinella, Giovanni Maria and Fuegen, Christian and Ghanem, Bernard and Ithapu, Vamsi Krishna and Jawahar, C. V. and Joo, Hanbyul and Kitani, Kris and Li, Haizhou and Newcombe, Richard and Oliva, Aude and Park, Hyun Soo and Rehg, James M. and Sato, Yoichi and Shi, Jianbo and Shou, Mike Zheng and Torralba, Antonio and Torresani, Lorenzo and Yan, Mingfei and Malik, Jitendra},
	month = mar,
	year = {2022},
	note = {arXiv:2110.07058 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
}

@misc{li_eye_2020,
	title = {In the {Eye} of the {Beholder}: {Gaze} and {Actions} in {First} {Person} {Video}},
	shorttitle = {In the {Eye} of the {Beholder}},
	url = {http://arxiv.org/abs/2006.00626},
	abstract = {We address the task of jointly determining what a person is doing and where they are looking based on the analysis of video captured by a headworn camera. To facilitate our research, we first introduce the EGTEA Gaze+ dataset. Our dataset comes with videos, gaze tracking data, hand masks and action annotations, thereby providing the most comprehensive benchmark for First Person Vision (FPV). Moving beyond the dataset, we propose a novel deep model for joint gaze estimation and action recognition in FPV. Our method describes the participant's gaze as a probabilistic variable and models its distribution using stochastic units in a deep network. We further sample from these stochastic units, generating an attention map to guide the aggregation of visual features for action recognition. Our method is evaluated on our EGTEA Gaze+ dataset and achieves a performance level that exceeds the state-of-the-art by a significant margin. More importantly, we demonstrate that our model can be applied to larger scale FPV dataset---EPIC-Kitchens even without using gaze, offering new state-of-the-art results on FPV action recognition.},
	
	publisher = {arXiv},
	author = {Li, Yin and Liu, Miao and Rehg, James M.},
	month = oct,
	year = {2020},
	note = {arXiv:2006.00626 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/mengze/Zotero/storage/CRYFL62N/Li et al. - 2020 - In the Eye of the Beholder Gaze and Actions in Fi.pdf:application/pdf;arXiv.org Snapshot:/Users/mengze/Zotero/storage/9HMU6UKM/2006.html:text/html},
}

@INPROCEEDINGS{Sun_Rev_2017,
  author={Sun, Chen and Shrivastava, Abhinav and Singh, Saurabh and Gupta, Abhinav},
  booktitle={2017 IEEE International Conference on Computer Vision (ICCV)}, 
  title={Revisiting Unreasonable Effectiveness of Data in Deep Learning Era}, 
  year={2017},
  pages={843-852},
  doi={10.1109/ICCV.2017.97}
  }

@misc{russakovsky_imagenet_2015,
	title = {{ImageNet} {Large} {Scale} {Visual} {Recognition} {Challenge}},
	url = {http://arxiv.org/abs/1409.0575},
	abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements.},
	
	publisher = {arXiv},
	author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
	month = jan,
	year = {2015},
	note = {arXiv:1409.0575 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, I.4.8, I.5.2},
	file = {arXiv Fulltext PDF:/Users/mengze/Zotero/storage/HQNP5B5V/Russakovsky et al. - 2015 - ImageNet Large Scale Visual Recognition Challenge.pdf:application/pdf;arXiv.org Snapshot:/Users/mengze/Zotero/storage/X6QMNLY5/1409.html:text/html},
}

@misc{kay_kinetics_2017,
	title = {The {Kinetics} {Human} {Action} {Video} {Dataset}},
	url = {http://arxiv.org/abs/1705.06950},
	abstract = {We describe the DeepMind Kinetics human action video dataset. The dataset contains 400 human action classes, with at least 400 video clips for each action. Each clip lasts around 10s and is taken from a different YouTube video. The actions are human focussed and cover a broad range of classes including human-object interactions such as playing instruments, as well as human-human interactions such as shaking hands. We describe the statistics of the dataset, how it was collected, and give some baseline performance figures for neural network architectures trained and tested for human action classification on this dataset. We also carry out a preliminary analysis of whether imbalance in the dataset leads to bias in the classifiers.},
	
	publisher = {arXiv},
	author = {Kay, Will and Carreira, Joao and Simonyan, Karen and Zhang, Brian and Hillier, Chloe and Vijayanarasimhan, Sudheendra and Viola, Fabio and Green, Tim and Back, Trevor and Natsev, Paul and Suleyman, Mustafa and Zisserman, Andrew},
	month = may,
	year = {2017},
	note = {arXiv:1705.06950 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/mengze/Zotero/storage/Z4EN24B3/Kay et al. - 2017 - The Kinetics Human Action Video Dataset.pdf:application/pdf;arXiv.org Snapshot:/Users/mengze/Zotero/storage/CM3S6TD5/1705.html:text/html},
}

@misc{neimark_video_2021,
	title = {Video {Transformer} {Network}},
	url = {http://arxiv.org/abs/2102.00719},
	abstract = {This paper presents VTN, a transformer-based framework for video recognition. Inspired by recent developments in vision transformers, we ditch the standard approach in video action recognition that relies on 3D ConvNets and introduce a method that classifies actions by attending to the entire video sequence information. Our approach is generic and builds on top of any given 2D spatial network. In terms of wall runtime, it trains \$16.1{\textbackslash}times\$ faster and runs \$5.1{\textbackslash}times\$ faster during inference while maintaining competitive accuracy compared to other state-of-the-art methods. It enables whole video analysis, via a single end-to-end pass, while requiring \$1.5{\textbackslash}times\$ fewer GFLOPs. We report competitive results on Kinetics-400 and present an ablation study of VTN properties and the trade-off between accuracy and inference speed. We hope our approach will serve as a new baseline and start a fresh line of research in the video recognition domain. Code and models are available at: https://github.com/bomri/SlowFast/blob/master/projects/vtn/README.md},
	
	publisher = {arXiv},
	author = {Neimark, Daniel and Bar, Omri and Zohar, Maya and Asselmann, Dotan},
	month = aug,
	year = {2021},
	note = {arXiv:2102.00719 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/mengze/Zotero/storage/CZ7AELIB/Neimark et al. - 2021 - Video Transformer Network.pdf:application/pdf;arXiv.org Snapshot:/Users/mengze/Zotero/storage/YEBXXGGL/2102.html:text/html},
}

@incollection{ferrari_compositional_2018,
	address = {Cham},
	title = {Compositional {Learning} for {Human} {Object} {Interaction}},
	volume = {11218},
	isbn = {978-3-030-01263-2 978-3-030-01264-9},
	url = {https://link.springer.com/10.1007/978-3-030-01264-9_15},
	abstract = {The world of human-object interactions is rich. While generally we sit on chairs and sofas, if need be we can even sit on TVs or top of shelves. In recent years, there has been progress in modeling actions and human-object interactions. However, most of these approaches require lots of data. It is not clear if the learned representations of actions are generalizable to new categories. In this paper, we explore the problem of zero-shot learning of human-object interactions. Given limited verb-noun interactions in training data, we want to learn a model than can work even on unseen combinations. To deal with this problem, In this paper, we propose a novel method using external knowledge graph and graph convolutional networks which learns how to compose classiﬁers for verbnoun pairs. We also provide benchmarks on several dataset for zero-shot learning including both image and video. We hope our method, dataset and baselines will facilitate future research in this direction.},
	language = {en},
	
	booktitle = {Computer {Vision} – {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Kato, Keizo and Li, Yin and Gupta, Abhinav},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	doi = {10.1007/978-3-030-01264-9_15},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {247--264},
	file = {Kato et al. - 2018 - Compositional Learning for Human Object Interactio.pdf:/Users/mengze/Zotero/storage/VSKNA9K2/Kato et al. - 2018 - Compositional Learning for Human Object Interactio.pdf:application/pdf},
}

@misc{baradel_object_2018,
	title = {Object {Level} {Visual} {Reasoning} in {Videos}},
	url = {http://arxiv.org/abs/1806.06157},
	abstract = {Human activity recognition is typically addressed by detecting key concepts like global and local motion, features related to object classes present in the scene, as well as features related to the global context. The next open challenges in activity recognition require a level of understanding that pushes beyond this and call for models with capabilities for fine distinction and detailed comprehension of interactions between actors and objects in a scene. We propose a model capable of learning to reason about semantically meaningful spatiotemporal interactions in videos. The key to our approach is a choice of performing this reasoning at the object level through the integration of state of the art object detection networks. This allows the model to learn detailed spatial interactions that exist at a semantic, object-interaction relevant level. We evaluate our method on three standard datasets (Twenty-BN Something-Something, VLOG and EPIC Kitchens) and achieve state of the art results on all of them. Finally, we show visualizations of the interactions learned by the model, which illustrate object classes and their interactions corresponding to different activity classes.},
	
	publisher = {arXiv},
	author = {Baradel, Fabien and Neverova, Natalia and Wolf, Christian and Mille, Julien and Mori, Greg},
	month = sep,
	year = {2018},
	note = {arXiv:1806.06157 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/mengze/Zotero/storage/M2PFHNDL/Baradel et al. - 2018 - Object Level Visual Reasoning in Videos.pdf:application/pdf;arXiv.org Snapshot:/Users/mengze/Zotero/storage/TZI3TG4L/1806.html:text/html},
}

@inproceedings{xu_learning_2019,
	address = {Long Beach, CA, USA},
	title = {Learning to {Detect} {Human}-{Object} {Interactions} {With} {Knowledge}},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8953301/},
	doi = {10.1109/CVPR.2019.00212},
	abstract = {The recent advances in instance-level detection tasks lay a strong foundation for automated visual scenes understanding. However, the ability to fully comprehend a social scene still eludes us. In this work, we focus on detecting human-object interactions (HOIs) in images, an essential step towards deeper scene understanding. HOI detection aims to localize human and objects, as well as to identify the complex interactions between them. Innate in practical problems with large label space, HOI categories exhibit a long-tail distribution, i.e., there exist some rare categories with very few training samples. Given the key observation that HOIs contain intrinsic semantic regularities despite they are visually diverse, we tackle the challenge of longtail HOI categories by modeling the underlying regularities among verbs and objects in HOIs as well as general relationships. In particular, we construct a knowledge graph based on the ground-truth annotations of training dataset and external source. In contrast to direct knowledge incorporation, we address the necessity of dynamic imagespeciﬁc knowledge retrieval by multi-modal learning, which leads to an enhanced semantic embedding space for HOI comprehension. The proposed method shows improved performance on V-COCO and HICO-DET benchmarks, especially when predicting the rare HOI categories.},
	language = {en},
	
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Xu, Bingjie and Wong, Yongkang and Li, Junnan and Zhao, Qi and Kankanhalli, Mohan S.},
	month = jun,
	year = {2019},
	pages = {2019--2028},
	file = {Xu et al. - 2019 - Learning to Detect Human-Object Interactions With .pdf:/Users/mengze/Zotero/storage/SDIEUARV/Xu et al. - 2019 - Learning to Detect Human-Object Interactions With .pdf:application/pdf},
}

@misc{huang_predicting_2018,
	title = {Predicting {Gaze} in {Egocentric} {Video} by {Learning} {Task}-dependent {Attention} {Transition}},
	url = {http://arxiv.org/abs/1803.09125},
	abstract = {We present a new computational model for gaze prediction in egocentric videos by exploring patterns in temporal shift of gaze fixations (attention transition) that are dependent on egocentric manipulation tasks. Our assumption is that the high-level context of how a task is completed in a certain way has a strong influence on attention transition and should be modeled for gaze prediction in natural dynamic scenes. Specifically, we propose a hybrid model based on deep neural networks which integrates task-dependent attention transition with bottom-up saliency prediction. In particular, the task-dependent attention transition is learned with a recurrent neural network to exploit the temporal context of gaze fixations, e.g. looking at a cup after moving gaze away from a grasped bottle. Experiments on public egocentric activity datasets show that our model significantly outperforms state-of-the-art gaze prediction methods and is able to learn meaningful transition of human attention.},
	
	publisher = {arXiv},
	author = {Huang, Yifei and Cai, Minjie and Li, Zhenqiang and Sato, Yoichi},
	month = dec,
	year = {2018},
	note = {arXiv:1803.09125 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/mengze/Zotero/storage/8RA8CPJU/Huang et al. - 2018 - Predicting Gaze in Egocentric Video by Learning Ta.pdf:application/pdf;arXiv.org Snapshot:/Users/mengze/Zotero/storage/FS7EE9VC/1803.html:text/html},
}

@inproceedings{tavakoli_digging_2019,
	address = {Waikoloa Village, HI, USA},
	title = {Digging {Deeper} {Into} {Egocentric} {Gaze} {Prediction}},
	isbn = {978-1-72811-975-5},
	url = {https://ieeexplore.ieee.org/document/8658619/},
	doi = {10.1109/WACV.2019.00035},
	
	booktitle = {2019 {IEEE} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	publisher = {IEEE},
	author = {Tavakoli, Hamed Rezazadegan and Rahtu, Esa and Kannala, Juho and Borji, Ali},
	month = jan,
	year = {2019},
	pages = {273--282},
	file = {Submitted Version:/Users/mengze/Zotero/storage/VW8PSLIE/Tavakoli et al. - 2019 - Digging Deeper Into Egocentric Gaze Prediction.pdf:application/pdf},
}


@article{land_roles_1999,
	title = {The {Roles} of {Vision} and {Eye} {Movements} in the {Control} of {Activities} of {Daily} {Living}},
	volume = {28},
	issn = {0301-0066, 1468-4233},
	url = {http://journals.sagepub.com/doi/10.1068/p2935},
	doi = {10.1068/p2935},
	abstract = {The aim of this study was to determine the pattern of fixations during the performance of a well-learned task in a natural setting (making tea), and to classify the types of monitoring action that the eyes perform. We used a head-mounted eye-movement video camera, which provided a continuous view of the scene ahead, with a dot indicating foveal direction with an accuracy of about 1 deg. A second video camera recorded the subject's activities from across the room. The videos were linked and analysed frame by frame. Foveal direction was always close to the object being manipulated, and very few fixations were irrelevant to the task. The first object-related fixation typically led the first indication of manipulation by 0.56 s, and vision moved to the next object about 0.61 s before manipulation of the previous object was complete. Each object-related act that did not involve a waiting period lasted an average of 3.3 s and involved about 7 fixations. Roughly a third of all fixations on objects could be definitely identified with one of four monitoring functions: locating objects used later in the process, directing the hand or object in the hand to a new location, guiding the approach of one object to another (eg kettle and lid), and checking the state of some variable (eg water level). We conclude that although the actions of tea-making are `automated' and proceed with little conscious involvement, the eyes closely monitor every step of the process. This type of unconscious attention must be a common phenomenon in everyday life.},
	language = {en},
	number = {11},
	urldate = {2023-11-21},
	journal = {Perception},
	author = {Land, Michael and Mennie, Neil and Rusted, Jennifer},
	month = nov,
	year = {1999},
	pages = {1311--1328},
	file = {Land et al. - 1999 - The Roles of Vision and Eye Movements in the Contr.pdf:/Users/mengze/Zotero/storage/US6MQIN3/Land et al. - 1999 - The Roles of Vision and Eye Movements in the Contr.pdf:application/pdf},
}


@misc{liu2021swin,
	title = {Swin {Transformer}: {Hierarchical} {Vision} {Transformer} using {Shifted} {Windows}},
	shorttitle = {Swin {Transformer}},
	url = {http://arxiv.org/abs/2103.14030},
	urldate = {2023-11-11},
	publisher = {arXiv},
	author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
	month = aug,
	year = {2021},
	note = {arXiv:2103.14030 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/mengze/Zotero/storage/26GU7KZH/Liu et al. - 2021 - Swin Transformer Hierarchical Vision Transformer .pdf:application/pdf;arXiv.org Snapshot:/Users/mengze/Zotero/storage/HMNHE4TC/2103.html:text/html},
}

@misc{zhu2020comprehensive,
    title={A Comprehensive Study of Deep Video Action Recognition}, 
    author={Yi Zhu and Xinyu Li and Chunhui Liu and Mohammadreza Zolfaghari and Yuanjun Xiong and Chongruo Wu and Zhi Zhang and Joseph Tighe and R. Manmatha and Mu Li},
    year={2020},
    eprint={2012.06567},
    archivePrefix={arXiv},
    url = {https://arxiv.org/pdf/2012.06567}
}

@inproceedings{karpathy_large-scale_2014,
	address = {Columbus, OH, USA},
	title = {Large-{Scale} {Video} {Classification} with {Convolutional} {Neural} {Networks}},
	isbn = {978-1-4799-5118-5},
	url = {https://ieeexplore.ieee.org/document/6909619},
	doi = {10.1109/CVPR.2014.223},
	urldate = {2024-06-21},
	booktitle = {2014 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Karpathy, Andrej and Toderici, George and Shetty, Sanketh and Leung, Thomas and Sukthankar, Rahul and Fei-Fei, Li},
	month = jun,
	year = {2014},
	pages = {1725--1732},
	file = {Submitted Version:/Users/mengze/Zotero/storage/789M5RY8/Karpathy et al. - 2014 - Large-Scale Video Classification with Convolutiona.pdf:application/pdf},
}

@article{WangXW0LTG16,
  author       = {Limin Wang and
                  Yuanjun Xiong and
                  Zhe Wang and
                  Yu Qiao and
                  Dahua Lin and
                  Xiaoou Tang and
                  Luc Van Gool},
  title        = {Temporal Segment Networks: Towards Good Practices for Deep Action
                  Recognition},
  journal      = {CoRR},
  volume       = {abs/1608.00859},
  year         = {2016},
  url          = {http://arxiv.org/abs/1608.00859},
  eprinttype    = {arXiv},
  eprint       = {1608.00859},
  timestamp    = {Wed, 11 Sep 2019 15:40:23 +0200}
}

@misc{lin_tsm_2019,
	title = {{TSM}: {Temporal} {Shift} {Module} for {Efficient} {Video} {Understanding}},
	shorttitle = {{TSM}},
	url = {http://arxiv.org/abs/1811.08383},
	abstract = {The explosive growth in video streaming gives rise to challenges on performing video understanding at high accuracy and low computation cost. Conventional 2D CNNs are computationally cheap but cannot capture temporal relationships; 3D CNN based methods can achieve good performance but are computationally intensive, making it expensive to deploy. In this paper, we propose a generic and effective Temporal Shift Module (TSM) that enjoys both high efficiency and high performance. Specifically, it can achieve the performance of 3D CNN but maintain 2D CNN's complexity. TSM shifts part of the channels along the temporal dimension; thus facilitate information exchanged among neighboring frames. It can be inserted into 2D CNNs to achieve temporal modeling at zero computation and zero parameters. We also extended TSM to online setting, which enables real-time low-latency online video recognition and video object detection. TSM is accurate and efficient: it ranks the first place on the Something-Something leaderboard upon publication; on Jetson Nano and Galaxy Note8, it achieves a low latency of 13ms and 35ms for online video recognition. The code is available at: https://github.com/mit-han-lab/temporal-shift-module.},
	urldate = {2024-06-21},
	publisher = {arXiv},
	author = {Lin, Ji and Gan, Chuang and Han, Song},
	month = aug,
	year = {2019},
	note = {arXiv:1811.08383 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/mengze/Zotero/storage/QCH48WXP/Lin et al. - 2019 - TSM Temporal Shift Module for Efficient Video Und.pdf:application/pdf;arXiv.org Snapshot:/Users/mengze/Zotero/storage/JJ94IZJ3/1811.html:text/html},
}

@article{wang_Non-local_2017,
  author       = {Xiaolong Wang and
                  Ross B. Girshick and
                  Abhinav Gupta and
                  Kaiming He},
  title        = {Non-local Neural Networks},
  journal      = {CoRR},
  volume       = {abs/1711.07971},
  year         = {2017},
  url          = {http://arxiv.org/abs/1711.07971},
  eprinttype    = {arXiv},
  eprint       = {1711.07971},
  timestamp    = {Fri, 05 Apr 2019 07:29:46 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1711-07971.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{carion_End-to-End_2020,
  author       = {Nicolas Carion and
                  Francisco Massa and
                  Gabriel Synnaeve and
                  Nicolas Usunier and
                  Alexander Kirillov and
                  Sergey Zagoruyko},
  title        = {End-to-End Object Detection with Transformers},
  journal      = {CoRR},
  volume       = {abs/2005.12872},
  year         = {2020},
  url          = {https://arxiv.org/abs/2005.12872},
  eprinttype    = {arXiv},
  eprint       = {2005.12872},
  timestamp    = {Thu, 28 May 2020 17:38:09 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2005-12872.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{Damen2018EPICKITCHENS,
   title={Scaling Egocentric Vision: The EPIC-KITCHENS Dataset},
   author={Damen, Dima and Doughty, Hazel and Farinella, Giovanni Maria  and Fidler, Sanja and 
           Furnari, Antonino and Kazakos, Evangelos and Moltisanti, Davide and Munro, Jonathan 
           and Perrett, Toby and Price, Will and Wray, Michael},
   booktitle={European Conference on Computer Vision (ECCV)},
   year={2018}
}

@misc{kingma_adam_2017,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	urldate = {2024-06-26},
	publisher = {arXiv},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = jan,
	year = {2017},
	note = {arXiv:1412.6980 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/mengze/Zotero/storage/U4XIUL6I/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf:application/pdf;arXiv.org Snapshot:/Users/mengze/Zotero/storage/CF93U9DC/1412.html:text/html},
}

@misc{loshchilov_decoupled_2019,
	title = {Decoupled {Weight} {Decay} {Regularization}},
	url = {http://arxiv.org/abs/1711.05101},
	urldate = {2024-06-26},
	publisher = {arXiv},
	author = {Loshchilov, Ilya and Hutter, Frank},
	month = jan,
	year = {2019},
	note = {arXiv:1711.05101 [cs, math]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Optimization and Control},
	file = {arXiv Fulltext PDF:/Users/mengze/Zotero/storage/4YD99QNU/Loshchilov and Hutter - 2019 - Decoupled Weight Decay Regularization.pdf:application/pdf;arXiv.org Snapshot:/Users/mengze/Zotero/storage/2TT9ITEF/1711.html:text/html},
}

@misc{ren_faster_2016,
	title = {Faster {R}-{CNN}: {Towards} {Real}-{Time} {Object} {Detection} with {Region} {Proposal} {Networks}},
	shorttitle = {Faster {R}-{CNN}},
	url = {http://arxiv.org/abs/1506.01497},
	urldate = {2024-07-02},
	publisher = {arXiv},
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	month = jan,
	year = {2016},
	note = {arXiv:1506.01497 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/mengze/Zotero/storage/SU6I87NM/Ren et al. - 2016 - Faster R-CNN Towards Real-Time Object Detection w.pdf:application/pdf;arXiv.org Snapshot:/Users/mengze/Zotero/storage/JP7ERQ9A/1506.html:text/html},
}

@article{niri2021machine,
  title={Machine learning for optimised and clean Li-ion battery manufacturing: Revealing the dependency between electrode and cell characteristics},
  author={Niri, Mona Faraji and Liu, Kailong and Apachitei, Geanina and Ramirez, Luis Roman and Lain, Michael and Widanage, Dhammika and Marco, James},
  journal={Journal of Cleaner Production},
  volume={324},
  pages={129272},
  year={2021},
  publisher={Elsevier}
}

@misc{suh2023workeractivityrecognitionmanufacturing,
      title={Worker Activity Recognition in Manufacturing Line Using Near-body Electric Field}, 
      author={Sungho Suh and Vitor Fortes Rey and Sizhen Bian and Yu-Chi Huang and Jože M. Rožanec and Hooman Tavakoli Ghinani and Bo Zhou and Paul Lukowicz},
      year={2023},
      eprint={2308.03514},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2308.03514}, 
}